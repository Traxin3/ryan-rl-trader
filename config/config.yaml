algorithm: sb3_ppo

# Model/Transformer hyperparameters
model:
  features_dim: 256
  transformer:
    d_model: 256
    nhead: 8
    num_layers: 4
    dropout: 0.1

# Environment Configuration
env:
  symbols: ["EURUSD"]
  timeframes: [15]
  window_size: 100
  reward_scaling: 2.0
  min_reward: -10.0
  max_reward: 10.0
  survival_bonus: 0.02
  leverage_penalty: 0.02
  trade_reward_multiplier: 2.0
  drawdown_penalty: 0.2
  early_close_bonus: 0.1
  volatility_penalty: 0.05
  position_size_penalty: 0.02
  max_leverage: 20.0
  min_tp_sl_ratio: 1.8
  atr_multiplier: 2.2
  use_cached_features: true
  execution_cost_penalty_weight: 0.1
  max_daily_drawdown: 0.2

# Stable-Baselines3 PPO hyperparameters
sb3_ppo:
  # parallel envs
  n_envs: 1           # set >1 for vectorized envs
  use_subproc: false  # true on Linux/macOS for SubprocVecEnv
  # training
  total_timesteps: 2000000
  n_steps: 2048
  batch_size: 256
  n_epochs: 10
  learning_rate: 0.0003
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.0
  vf_coef: 0.5
  max_grad_norm: 0.5
  # checkpointing
  checkpoint_every_steps: 200000
  model_path: ppo_transformer_mtsim_final

# Global stop criteria retained for compatibility with CLI wrappers
stop:
  timesteps_total: 2000000