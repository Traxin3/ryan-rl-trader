{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad1796d",
   "metadata": {},
   "source": [
    "# Project Ryan: RL Trading Environment (Colab Starter)\n",
    "\n",
    "Welcome to the Colab notebook for Project Ryan! This notebook will guide you through setting up, training, and evaluating the RL trading agent using the custom gym-mtsim-inspired environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e40a6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 1: Install Required Packages\n",
    "!pip install gymnasium stable-baselines3[extra] MetaTrader5 tqdm pandas numpy matplotlib plotly pathos scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5513ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if MetaTrader5 is available (for Colab/cloud compatibility)\n",
    "import platform\n",
    "MT5_AVAILABLE = False\n",
    "if platform.system() == \"Windows\":\n",
    "    try:\n",
    "        import MetaTrader5 as mt5\n",
    "        MT5_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        print(\"MetaTrader5 not installed. Will use local cache or skip MT5 features.\")\n",
    "else:\n",
    "    print(\"MetaTrader5 is only available on Windows. Running in cloud/offline mode.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3d334d",
   "metadata": {},
   "source": [
    "## Section 2: Clone the Project Repository\n",
    "\n",
    "Clone the Project Ryan codebase from GitHub. If you have your own fork, update the URL below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c3ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the Project Ryan repo (replace with your fork if needed)\n",
    "!git clone https://github.com/Traxin3/ryan-rl-trader.git\n",
    "%cd ryan-rl-trader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae2a33",
   "metadata": {},
   "source": [
    "## Section 3: Set Up Environment Variables and Configuration\n",
    "\n",
    "Edit the configuration below to set your trading symbols, timeframes, and other environment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b628035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your environment configuration here\n",
    "SYMBOLS = ['EURUSD']\n",
    "TIMEFRAMES = [1, 5, 15]\n",
    "WINDOW_SIZE = 30\n",
    "SYMBOL_MAX_ORDERS = 2\n",
    "REWARD_SCALING = 1.0\n",
    "MAX_LEVERAGE = 5.0\n",
    "RISK_ADJUSTED_REWARD = True\n",
    "EARLY_CLOSE_BONUS = 0.1\n",
    "DIVERSIFICATION_BONUS = 0.05\n",
    "# Add more config as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87098d9c",
   "metadata": {},
   "source": [
    "## Section 4: Download or Prepare Market Data\n",
    "\n",
    "Download historical market data using the provided scripts. Data will be cached locally for fast reloads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6defccd9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3546de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download or prepare market data\n",
    "# If your codebase has a script for this, run it here. Otherwise, data will be downloaded automatically on first run.\n",
    "# Example (uncomment if you have a script):\n",
    "# !python scripts/download_data.py --symbols EURUSD --timeframes 1 5 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cea6dd",
   "metadata": {},
   "source": [
    "## Section 5: Run Training Script\n",
    "\n",
    "Run the main training script to start training the RL agent in the custom environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd89a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the main training script\n",
    "!python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c5912f",
   "metadata": {},
   "source": [
    "## Section 6: Monitor Training with TensorBoard\n",
    "\n",
    "Launch TensorBoard below to monitor training progress and view logged metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01393c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch TensorBoard in Colab\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./tensorboard_logs/ --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2455c7f",
   "metadata": {},
   "source": [
    "## Section 7: Evaluate and Visualize Results\n",
    "\n",
    "After training, run the evaluation script or use the code below to analyze agent performance and visualize results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f8512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate and visualize results after training\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "\n",
    "# Reload the trained model\n",
    "model = PPO.load(\"ppo_transformer_mtsim_final\")\n",
    "\n",
    "# You may need to re-create the environment as in main.py\n",
    "from gym_mtsim.envs.mt_env import MtEnv\n",
    "venv = MtEnv(\n",
    "    symbols=SYMBOLS,\n",
    "    timeframes=TIMEFRAMES,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    symbol_max_orders=SYMBOL_MAX_ORDERS,\n",
    "    reward_scaling=REWARD_SCALING,\n",
    "    max_leverage=MAX_LEVERAGE,\n",
    "    risk_adjusted_reward=RISK_ADJUSTED_REWARD,\n",
    "    early_close_bonus=EARLY_CLOSE_BONUS,\n",
    "    diversification_bonus=DIVERSIFICATION_BONUS\n",
    ")\n",
    "obs = venv.reset()\n",
    "rewards = []\n",
    "steps = 0\n",
    "\n",
    "while True:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = venv.step(action)\n",
    "    rewards.append(reward)\n",
    "    steps += 1\n",
    "    if done or truncated or steps >= 10000:\n",
    "        break\n",
    "\n",
    "print(f\"Evaluation completed. Total reward: {sum(rewards):.2f} over {steps} steps\")\n",
    "print(f\"Average reward per step: {np.mean(rewards):.4f}\")\n",
    "\n",
    "# Optionally, visualize performance (if supported)\n",
    "try:\n",
    "    venv.render(mode='advanced_figure')\n",
    "except Exception as e:\n",
    "    print(f\"Rendering failed: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
